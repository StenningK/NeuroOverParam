{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f828a801",
   "metadata": {},
   "source": [
    "### META-LEARNING SIMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96360f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device='cuda'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50680af7",
   "metadata": {},
   "source": [
    "### LOADING THE DATA\n",
    "\n",
    "The cell below loads the devices responses. If Single_R=False, the data will be loaded from all the devices in the physical neural network, if Single_R=True, the code will load the data from a single device.\n",
    "\n",
    "The PNN is driven by an external sinuoisodal signal $$s(t)=\\sin(\\omega t) $$, and it is tasked to reproduce different outputs $y_i(t)=a_i\\sin(\\omega_i t+\\theta_i)$ in a few-shot learning fashion and as described in the paper.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301aafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder='Sine_hierachies'\n",
    "X=[]\n",
    "T_data=250\n",
    "X=np.zeros([T_data,1])\n",
    "S=np.zeros([T_data,1])\n",
    "\n",
    "Single_R=False\n",
    "\n",
    "if Single_R==True:\n",
    "        \n",
    "        ## Loading signle device response\n",
    "        X1=np.load(os.path.join(datafolder,'7.ASVI04_From_HDS_Mem_peak_262.npy'),allow_pickle=True) \n",
    "        \n",
    "else:\n",
    "    \n",
    "    \n",
    "    ## Loading physical neural network response\n",
    "    for file in os.listdir(datafolder):\n",
    "\n",
    "        if '.npy' in file: \n",
    "\n",
    "            X1=np.load(os.path.join(datafolder,file),allow_pickle=True)\n",
    "            if np.shape(X1)[0]>=T_data and file !='all_targets.npy' and file!='target_names.npy' and file!='3.Pinwheel_ASVI_y.npy':\n",
    "\n",
    "                X=np.concatenate([X,X1[0:T_data,1:]],1)  \n",
    "\n",
    "S1=np.load(os.path.join(datafolder,'3.Pinwheel_ASVI_y.npy'),allow_pickle=True)\n",
    "X=np.concatenate([X,S1[0:T_data,2:]],1)\n",
    "\n",
    "S=S1[0:T_data,0]\n",
    "S=(S-np.min(S))/(np.max(S)-np.min(S))\n",
    "\n",
    "\n",
    "## Plot external Signal and response example\n",
    "fig, ax = plt.subplots(1, 2,figsize=(10, 4))\n",
    "\n",
    "ax[0].plot(S[0:100],'.')\n",
    "dts=torch.arange(0,250)*2*np.pi/30\n",
    "ax[0].plot((torch.sin(dts[0:100])+1)/2)\n",
    "ax[0].set_xlabel('steps')\n",
    "ax[0].set_ylabel('External signal')\n",
    "ax[0].spines['right'].set_visible(False)\n",
    "ax[0].spines['top'].set_visible(False)\n",
    "\n",
    "\n",
    "X=torch.tensor(X[:,1:]).float().to('cuda')\n",
    "X=X-torch.min(X,0)[0].unsqueeze(0)\n",
    "X_M=torch.max(torch.abs(X),0)[0]\n",
    "X=X/X_M.unsqueeze(0)\n",
    "\n",
    "Dim_plot=50\n",
    "rand_dims=np.random.randint(0,X.size()[0],Dim_plot)\n",
    "\n",
    "for n in range(Dim_plot):\n",
    "    \n",
    "    \n",
    "    ax[1].plot(X[0:100,rand_dims[n]].to('cpu'))\n",
    "\n",
    "ax[1].set_xlabel('steps')\n",
    "ax[1].set_ylabel('Devices Responses')    \n",
    "ax[1].spines['right'].set_visible(False)\n",
    "ax[1].spines['top'].set_visible(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af39966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta_ReadOut(nn.Module):\n",
    "    \n",
    "    def __init__(self,Ns,t_in,eta):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## Number of layers in the network...it is always 1 for the case studeied, given that \n",
    "        ## are training a read-out from the devices responses\n",
    "        N_layers=np.shape(Ns)[0]\n",
    "        \n",
    "        ## Lists of parameters\n",
    "        self.Ws=[]\n",
    "        self.bs=[]\n",
    "        \n",
    "        self.eta=eta\n",
    "        \n",
    "        self.t_in=t_in\n",
    "        self.etas_bs=[]\n",
    "        self.etas_Ws=[]\n",
    "                \n",
    "        self.Ws.append(nn.Parameter(torch.rand([Ns[0],Ns[1]],device=device)/(Ns[0]+Ns[1])))\n",
    "        \n",
    "        ## Multiplicative factor to each separate output weight matrix\n",
    "        ## these are the variable that will change in the inner meta-learning loop\n",
    "        self.W_out=[nn.Parameter(torch.ones([Ns[1]],device=device)*0.01)]\n",
    "        \n",
    "        self.bs=[nn.Parameter(torch.zeros([Ns[1]],device=device))]\n",
    "        \n",
    "        \n",
    "        self.loss=nn.MSELoss()\n",
    "        self.Ns=Ns\n",
    "        \n",
    "        \n",
    "    def Initialise_Hyperparameters(self,eta_Meta,c,N_iter,t_in):\n",
    "        \n",
    "        \n",
    "        ## Hyperparameterers\n",
    "        self.eta_Meta=eta_Meta ## Learning rate for the Meta-update\n",
    "        \n",
    "        self.opt=optim.Adam([{ 'params': self.Ws, 'lr':eta_Meta }])\n",
    "        self.c=c\n",
    "        \n",
    "        self.N_iter=N_iter \n",
    "        self.E=torch.zeros([N_iter,t_in]) ## Inner loop loss function\n",
    "        self.L=torch.zeros([N_iter,t_in]) ## Outer loop (meta) loss function\n",
    "        self.counter=0 \n",
    "        self.w_t=torch.ones([t_in]) ## Weights to the different loss functions, computed at different steps of the \n",
    "                                    ## inner loop.\n",
    "                                    ## In our simulations, it will be always zero but for the last step\n",
    "        self.t_steps=t_in\n",
    "        \n",
    "    \n",
    "    \n",
    "    ## Computation of the output activities\n",
    "    def Forward(self, S, Target, Ws, bs):\n",
    "        \n",
    "        xs=[]\n",
    "        xs.append(S)\n",
    "        \n",
    "        Out= torch.add(torch.matmul(xs[0],Ws[0])*Ws[1],bs[0])\n",
    "                \n",
    "        E= self.loss(Out,Target)\n",
    "                            \n",
    "        return E, Out, xs[-1]\n",
    "    \n",
    "    \n",
    "    ## Meta-learning step\n",
    "    def Meta(self, S_T, T_T, S_q, T_q, t_steps):\n",
    "        \n",
    "        N_task=len(S_T)\n",
    "        \n",
    "        # Initialisation of the loss functions, where N_task is the number of tasks considered \n",
    "        # for training and t_steps are the number of steps accomplished in the inner loop\n",
    "        E_n=torch.zeros([N_task,t_steps])\n",
    "        L=torch.zeros([N_task,t_steps])\n",
    "            \n",
    "        W_out=self.W_out\n",
    "        \n",
    "        ## For each different task\n",
    "        for n in range(N_task):\n",
    "            \n",
    "            ## First step\n",
    "            E,_,_=self.Forward(S_T[n],T_T[n], self.Ws+W_out, self.bs) ## Inner loss function\n",
    "        \n",
    "            Grads_W=torch.autograd.grad(E, self.W_out, retain_graph=True)  ## Corresponding gradients\n",
    "            \n",
    "            W_out = list(map(lambda p: p[1] - self.eta * p[0], zip(Grads_W, W_out) )) ## Update of the output scaling factors\n",
    "            \n",
    "            L[n,0],_,_=self.Forward(S_q[n],T_q[n], self.Ws+W_out, self.bs) ## Compuation of the meta objective\n",
    "            E_n[n,0]=E\n",
    "            \n",
    "            ## Other steps of the inner loop\n",
    "            for t in range(1,t_steps):\n",
    "                \n",
    "                E,_,_=self.Forward(S_T[n],T_T[n], self.Ws+W_out, self.bs)   ## Same as before\n",
    "        \n",
    "                Grads_W=torch.autograd.grad(E, W_out, retain_graph=True)\n",
    "                                \n",
    "                W_out = list(map(lambda p: p[1] - self.eta * p[0], zip(Grads_W, W_out) ))\n",
    "\n",
    "                L[n,t],_,_=self.Forward(S_q[n],T_q[n], self.Ws+W_out, self.bs)\n",
    "                E_n[n,t]=E\n",
    "        \n",
    "        \n",
    "        self.E[self.counter,:]=torch.mean(E_n.detach(),0)\n",
    "        self.L[self.counter,:]=torch.mean(L.detach(),0)\n",
    "        \n",
    "        Train=self.Check_Errors()\n",
    "        self.counter+=1\n",
    "        \n",
    "        No_Meta=False ## If this is True, it will perform simple perform gradient descent on the Error function \n",
    "                      ## in the first step\n",
    "        \n",
    "        \n",
    "        if No_Meta:\n",
    "            \n",
    "            E=torch.sum(E_n[n,0])/N_task\n",
    "            E.backward()\n",
    "            \n",
    "            self.opt.step()\n",
    "            \n",
    "            self.opt.zero_grad()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            E_meta=torch.sum(L*self.w_t.unsqueeze(0))/N_task\n",
    "            E_meta.backward()\n",
    "                        \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        \n",
    "        return E_n, L\n",
    "    \n",
    "    ## Class used to update the scaling factors of the error functions computed at the different steps of the inner loop\n",
    "    ## In general, these factors can decay as explained in the paper \"How to train your MAML\". \n",
    "    ## In our setting, given that we are exploiting a first order MAML on a simple read-out, we will set only the\n",
    "    ## final scaling factor to 1, and the resto to 0\n",
    "    def Check_Errors(self):\n",
    "        \n",
    "        Train=True\n",
    "        N_change=0\n",
    "        if self.counter>=N_change:\n",
    "            self.w_t[0:self.t_steps-1]=self.w_t[0:self.t_steps-1]*0 \n",
    "        \n",
    "        return Train\n",
    "    \n",
    "    ## Final few-shot learning adaptation\n",
    "    def Fine_Tuning(self, S, Target, n_steps, x_data, y_data):\n",
    "        \n",
    "        E_n=torch.zeros([self.t_in+n_steps])\n",
    "        E_all=torch.zeros([self.t_in+n_steps])\n",
    "        Y_data=torch.zeros([self.t_in+n_steps,x_data.size()[0],self.Ns[-1]]).to('cpu')\n",
    "        \n",
    "        W_out=self.W_out\n",
    "        Ws=self.Ws\n",
    "        \n",
    "        for n in range(self.t_in):\n",
    "            \n",
    "            \n",
    "            e_all,y,z=self.Forward(x_data, y_data, Ws+W_out, self.bs)\n",
    "            \n",
    "            Y_data[n,:,:]=y.detach().to('cpu')\n",
    "            \n",
    "            E,_,_=self.Forward(S,Target, self.Ws+W_out, self.bs)\n",
    "\n",
    "            Grads_W=torch.autograd.grad(E, W_out)\n",
    "                        \n",
    "            W_out = list(map(lambda p: p[1] - self.eta * p[0], zip(Grads_W, W_out)))\n",
    "            \n",
    "            E_n[n]=E.detach()\n",
    "            E_all[n]=e_all.detach()\n",
    "        \n",
    "            \n",
    "        for n in range(self.t_in,n_steps+self.t_in):\n",
    "            \n",
    "            e_all,y,z=self.Forward(x_data, y_data, Ws+W_out, self.bs)\n",
    "            \n",
    "            Y_data[n,:,:]=y.detach().to('cpu')\n",
    "            \n",
    "            E,_,_=self.Forward(S,Target, Ws+W_out, self.bs)\n",
    "\n",
    "            Grads_W=torch.autograd.grad(E, Ws)\n",
    "                        \n",
    "            Ws = list(map(lambda p: p[1] - 0.0002 * p[0], zip(Grads_W, Ws)))\n",
    "            \n",
    "            E_n[n]=E.detach()\n",
    "            E_all[n]=e_all.detach()\n",
    "        \n",
    "        return E_n, E_all, Y_data\n",
    "    \n",
    "    \n",
    "    def Analysis(self, Ss, Targets, n_steps, x_datas, y_datas):\n",
    "        \n",
    "        N_task=len(Ss)\n",
    "        \n",
    "        YS=torch.zeros([N_task,self.t_in+n_steps,x_datas[0].size()[0],self.Ns[-1]]).to('cpu')\n",
    "        ES=torch.zeros([N_task,self.t_in+n_steps]).to('cpu')\n",
    "        ES_all=torch.zeros([N_task,self.t_in+n_steps]).to('cpu')\n",
    "        \n",
    "        for n in range(N_task):\n",
    "            \n",
    "            ES[n,:], ES_all[n,:], YS[n,:,:,:]= self.Fine_Tuning(Ss[n], Targets[n], n_steps, x_datas[n], y_datas[n])\n",
    "\n",
    "            \n",
    "        return ES, ES_all, YS    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424906d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Data Manager to generate the data \n",
    "\n",
    "class SineWaves:\n",
    "    \n",
    "    def __init__(self,dts,X):\n",
    "        \n",
    "        self.N_t=dts.size()[0]\n",
    "        self.dts=dts\n",
    "        \n",
    "        ## Hyperparameters defining the tasks distribution\n",
    "        ## Range of amplitudes used\n",
    "        self.As=[[-1.2,1.2],[-1.2,1.2],[-1.2,1.2],[-1.2,1.2],[-1.2,1.2]]\n",
    "        \n",
    "        ## Range of frequencies used\n",
    "        self.phases=[[0,np.pi],[0,np.pi/2],[0,np.pi/2],[0,np.pi/3],[0,np.pi/4]]\n",
    "        \n",
    "        self.N_F=5\n",
    "        self.X=X\n",
    "    \n",
    "    \n",
    "    ## The method samples data fot the updates for a specific task, where batch_size1 (batch_size2) is the\n",
    "    ## number of data points to update the parameters in the inner loop (for the meta-objective)\n",
    "    ## if Rand=True, the training data are sampled randomly, if Rand=False, the training data will be equally spaced\n",
    "    ## by the value in space\n",
    "    def Sample(self,batch_size1,batch_size2,space,Rand):\n",
    "        \n",
    "        \n",
    "        a=torch.zeros([self.N_F])\n",
    "        theta=torch.zeros([self.N_F])\n",
    "        \n",
    "        \n",
    "        ## Selecting the index of the data to be used\n",
    "        if Rand==True:\n",
    "            \n",
    "            n_t=np.random.randint(0,self.X.size()[0],batch_size1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            lin=np.arange(0,batch_size1)*space\n",
    "\n",
    "            n_t=np.random.randint(0,self.X.size()[0]-lin[-1])+lin\n",
    "\n",
    "        ns=np.delete(np.arange(0,self.X.size()[0]),n_t)\n",
    "        \n",
    "        ns=np.random.permutation(ns)\n",
    "        \n",
    "        ## Sampling the corresponding data, Xs_t, Xs_q\n",
    "        xs_t=self.dts[n_t]\n",
    "        Xs_t=self.X[n_t,:]\n",
    "                \n",
    "        n_q=ns[0:batch_size2]\n",
    "        xs_q=self.dts[n_q]\n",
    "        Xs_q=self.X[n_q,:]\n",
    "        \n",
    "        s_t=torch.zeros([batch_size1,self.N_F])\n",
    "        s_q=torch.zeros([batch_size2,self.N_F])\n",
    "        \n",
    "        ## Sampling a task and the corresponding targets\n",
    "        for n in range(self.N_F):\n",
    "            \n",
    "            \n",
    "            a[n]=self.As[n][0]+torch.rand(1)*(self.As[n][1]-self.As[n][0])            \n",
    "            theta[n]=self.phases[n][0]+torch.rand(1)*(self.phases[n][1]-self.phases[n][0])\n",
    "        \n",
    "            s_t[:,n]=a[n]*(torch.sin((n+1)*xs_t+theta[n])+1)/2\n",
    "            s_q[:,n]=a[n]*(torch.sin((n+1)*xs_q+theta[n])+1)/2\n",
    "            \n",
    "        \n",
    "        return s_t.to('cuda'), Xs_t.to('cuda'), n_t, s_q.to('cuda'), Xs_q.to('cuda'), n_q, a, theta\n",
    "    \n",
    "    ## The following method samples data for N_task \n",
    "    def Sample_Tr(self,N_task,batch_size1,batch_size2,space,Rand):\n",
    "        \n",
    "        T_T=[]\n",
    "        S_T=[]\n",
    "        T_q=[]\n",
    "        S_q=[]\n",
    "        N_T=[]\n",
    "        N_Q=[]\n",
    "        As=[]\n",
    "        Ths=[]\n",
    "        \n",
    "        for n in range(N_task):\n",
    "            \n",
    "            s_t,xs_t,n_t,s_q,xs_q,n_q,a,theta=Sines.Sample(batch_size1,batch_size2,space,Rand)\n",
    "            \n",
    "            T_T.append(s_t)\n",
    "            S_T.append(xs_t)\n",
    "            T_q.append(s_q)\n",
    "            S_q.append(xs_q)\n",
    "            \n",
    "            N_T.append(n_t)\n",
    "            N_Q.append(n_q)\n",
    "            \n",
    "            As.append(a)\n",
    "            Ths.append(theta)\n",
    "\n",
    "            \n",
    "        return T_T, S_T, N_T, T_q, S_q, N_Q, As, Ths\n",
    "    \n",
    "    ## Sample a task with the given values a for the amplitude and theta for the phase\n",
    "    def Sample_1(self,a,theta,batch_size,space,Rand=True):\n",
    "        \n",
    "        if Rand==True:\n",
    "            \n",
    "            n_t=np.random.randint(0,self.X.size()[0],batch_size)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            lin=np.arange(0,batch_size)*space\n",
    "\n",
    "            n_t=np.random.randint(0,self.X.size()[0]-lin[-1])+lin\n",
    "\n",
    "        S_T=self.X[n_t,:]\n",
    "        T_T=torch.zeros([batch_size,self.N_F])\n",
    "        y_data=torch.zeros([self.X.size()[0],self.N_F])\n",
    "        \n",
    "        for n in range(self.N_F):\n",
    "        \n",
    "            T_T[:,n]=a[n]*(torch.sin((n+1)*self.dts[n_t]+theta[n])+1)/2\n",
    "            y_data[:,n]=a[n]*(torch.sin((n+1)*self.dts+theta[n])+1)/2\n",
    "            \n",
    "\n",
    "        x_data=self.X[:,:]\n",
    "        \n",
    "        return S_T, T_T.to('cuda'), x_data, y_data.to('cuda'), n_t\n",
    "    \n",
    "    def sample_all(self,N_task,batch_size,space,Rand):\n",
    "        \n",
    "        Ss=[]\n",
    "        Targets=[]\n",
    "        x_datas=[]\n",
    "        y_datas=[]\n",
    "            \n",
    "        N_T=[]\n",
    "        As=[]\n",
    "        Ths=[]\n",
    "        Os=[]\n",
    "        \n",
    "        for n in range(N_task):\n",
    "            \n",
    "            a=torch.zeros([self.N_F])\n",
    "            theta=torch.zeros([self.N_F])\n",
    "            \n",
    "            for l in range(self.N_F):\n",
    "            \n",
    "                a[l]=self.As[l][0]+torch.rand(1)*(self.As[l][1]-self.As[l][0])            \n",
    "                theta[l]=self.phases[l][0]+torch.rand(1)*(self.phases[l][1]-self.phases[l][0])\n",
    "        \n",
    "            S_T, T_T, x_data, y_data, n_t=self.Sample_1(a,theta,batch_size,space,Rand)\n",
    "            \n",
    "            Ss.append(S_T)\n",
    "            Targets.append(T_T)\n",
    "            x_datas.append(x_data)\n",
    "            y_datas.append(y_data)\n",
    "            N_T.append(n_t)\n",
    "            As.append(a)\n",
    "            Ths.append(theta)\n",
    "        \n",
    "        return Ss, Targets, N_T, x_datas, y_datas, As, Ths\n",
    "\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sines=SineWaves(dts,X)\n",
    "space=3\n",
    "N_task=10\n",
    "rand_sample=True\n",
    "Rand=True\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "colors = mpl.colormaps['magma'].resampled(Sines.N_F)\n",
    "\n",
    "\n",
    "## Randomly sample a sine wave in the range of values considered \n",
    "if rand_sample==True:\n",
    "    \n",
    "    a=np.zeros([Sines.N_F])\n",
    "    theta=np.zeros([Sines.N_F])\n",
    "    \n",
    "    for n in range(Sines.N_F):\n",
    "        \n",
    "        a[n]=np.random.uniform(Sines.As[n][0],Sines.As[n][1])\n",
    "        theta[n]=np.random.uniform(Sines.phases[n][0],Sines.phases[n][1])\n",
    "\n",
    "batch_size1=10\n",
    "batch_size2=10\n",
    "\n",
    "fig, ax = plt.subplots(4, 1,figsize=(10, 12))\n",
    "\n",
    "S_T, T_T, x_data, y_data, n_t=Sines.Sample_1(a,theta,batch_size1,space)\n",
    "\n",
    "## Plot thdata \n",
    "\n",
    "for  n in range(Sines.N_F-1):\n",
    "    if n==0:\n",
    "        ax[0].plot(dts[n_t].to('cpu'),T_T[:,n].to('cpu'),'.',markersize=10,color='black')\n",
    "    \n",
    "    ax[0].plot(dts.to('cpu'),y_data[:,n].to('cpu'),color=colors(n))\n",
    "\n",
    "ax[0].set_xlabel('steps')\n",
    "ax[0].set_ylabel('Targets for different frequencies')\n",
    "ax[0].spines['right'].set_visible(False)\n",
    "ax[0].spines['top'].set_visible(False)\n",
    "ax[0].set_xlim(0, 30)\n",
    "ax[0].set_title('Example 1')\n",
    "    \n",
    "T_T, S_T, N_t, T_q, S_q, N_q, As, Ths=Sines.Sample_Tr(N_task,batch_size1,batch_size2,space,Rand)\n",
    "\n",
    "n_task=np.random.randint(0,N_task)\n",
    "for n in range(2,Sines.N_F):\n",
    "    if n==2:\n",
    "        ax[1].plot(dts[N_t[n_task]].to('cpu'), T_T[n_task][:,n].to('cpu'), '.',markersize=8,color='black')\n",
    "        ax[1].plot(dts[N_q[n_task]].to('cpu'), T_q[n_task][:,n].to('cpu'), '.',markersize=8,color='red')\n",
    "    Y=As[n_task][n]*(torch.sin((n+1)*dts+Ths[n_task][n])+1)/2\n",
    "    ax[1].plot(dts.to('cpu'), Y.to('cpu'),color=colors(n))\n",
    "\n",
    "\n",
    "    \n",
    "ax[1].set_xlabel('steps')\n",
    "ax[1].set_ylabel('Targets for different frequencies')\n",
    "ax[1].spines['right'].set_visible(False)\n",
    "ax[1].spines['top'].set_visible(False)\n",
    "ax[1].set_xlim(0, 30)\n",
    "ax[1].set_title('Example 2')\n",
    "\n",
    "#print(As[n_task], Ths[n_task])\n",
    "\n",
    "Ss, Targets, N_T, x_datas, y_datas, As, Ths=Sines.sample_all(N_task,batch_size1,space,Rand)\n",
    "\n",
    "Y1=torch.zeros([dts.size()[0]])\n",
    "for n in range(0,Sines.N_F):\n",
    "    if n==0:\n",
    "        ax[2].plot(dts[N_T[n_task]].to('cpu'), Targets[n_task][:,n].to('cpu'), '.',markersize=8,color='black')\n",
    "    Y=As[n_task][n]*(torch.sin((n+1)*dts+Ths[n_task][n])+1)/2\n",
    "    Y1=Y1+Y\n",
    "    ax[2].plot(dts.to('cpu'), Y.to('cpu'),color=colors(n))\n",
    "\n",
    "ax[2].set_xlabel('steps')\n",
    "ax[2].set_ylabel('Targets for different frequencies')\n",
    "ax[2].spines['right'].set_visible(False)\n",
    "ax[2].spines['top'].set_visible(False)    \n",
    "ax[2].set_xlim(0, 30)\n",
    "ax[2].set_title('Example 3')\n",
    "\n",
    "#print(As[n_task], Ths[n_task])\n",
    "\n",
    "ax[3].plot(dts.to('cpu'),Y1.to('cpu'))\n",
    "ax[3].set_xlabel('steps')\n",
    "ax[3].set_ylabel('Example of overall target')\n",
    "ax[3].spines['right'].set_visible(False)\n",
    "ax[3].spines['top'].set_visible(False)    \n",
    "ax[3].set_xlim(0, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3198243",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns=torch.tensor([X.size()[1],5,Sines.N_F])\n",
    "\n",
    "K=15\n",
    "Q=20\n",
    "N_task=15\n",
    "eta=1\n",
    "eta_meta=0.000005\n",
    "N_train=10000\n",
    "t_steps=10\n",
    "c=10\n",
    "space=3\n",
    "\n",
    "MLP=Meta_ReadOut(Ns,t_steps,eta)\n",
    "\n",
    "MLP.Initialise_Hyperparameters(eta_meta,c,N_train,t_steps)\n",
    "\n",
    "Sines=SineWaves(dts,X)\n",
    "\n",
    "E_train=torch.zeros([N_train,N_task,t_steps])\n",
    "L_train=torch.zeros([N_train,N_task,t_steps])\n",
    "n_steps=1000\n",
    "\n",
    "for n in range(N_train):\n",
    "    \n",
    "\n",
    "    T_T, S_T, N_t, T_q, S_q, N_q, As, Ths=Sines.Sample_Tr(N_task,K,Q,space,Rand=False)\n",
    "    \n",
    "    e_n, l=MLP.Meta(S_T, T_T, S_q, T_q, t_steps)\n",
    "    \n",
    "    E_train[n,:,:]=e_n[:,:].detach()\n",
    "    L_train[n,:,:]=l[:,:].detach()    \n",
    "    \n",
    "    if n%500==0:\n",
    "        \n",
    "        Ss, Targets, N_T, x_datas, y_datas, As, Ths=Sines.sample_all(N_task*3,K,space,Rand=True)\n",
    "        ES_tr, ES_all_tr, Ys_tr=MLP.Analysis(Ss, Targets, n_steps, x_datas, y_datas)\n",
    "        \n",
    "        print(n,'Performance:')\n",
    "        \n",
    "        ## Error on all the wave; it will give an idea of the generalisation ability over data not seen in\n",
    "        ## the inner loop\n",
    "        print('On all data:', torch.mean(ES_all_tr[0:900],0))\n",
    "        \n",
    "        ## Error on inner loop data; \n",
    "        ## The discrepancy between the two metrics gives an idea of overfitting tendency\n",
    "        print('On data seen during training:', torch.mean(ES_tr[0:900],0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "\n",
    "n_points=15\n",
    "space=2\n",
    "\n",
    "\n",
    "rand_sample=True\n",
    "if rand_sample==True:\n",
    "    \n",
    "    a=np.zeros([Sines.N_F])\n",
    "    theta=np.zeros([Sines.N_F])\n",
    "    \n",
    "    for n in range(Sines.N_F):\n",
    "        \n",
    "        a[n]=np.random.uniform(Sines.As[n][0],Sines.As[n][1])\n",
    "        theta[n]=np.random.uniform(Sines.phases[n][0],Sines.phases[n][1])\n",
    "        \n",
    "    \n",
    "xs_t, s_t, x_data, y_data, n_t=Sines.Sample_1(a,theta,n_points,space,Rand=True)\n",
    "\n",
    "n_steps=1000\n",
    "\n",
    "E_n, E_all, Y_data=MLP.Fine_Tuning(xs_t, s_t, n_steps, x_data.to('cuda'), y_data.to('cuda'))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(6, 1,figsize=(12, 15))\n",
    "\n",
    "print('Different colors correspond to the network output after siverse nuber of updates')\n",
    "x_lim=10\n",
    "X_lim=50\n",
    "for n in range(Sines.N_F):\n",
    "    \n",
    "    ax[n].plot(dts,y_data[:,n].detach().to('cpu'),'black')\n",
    "    ax[n].plot(dts,Y_data[2,:,n].detach().to('cpu'),'--')\n",
    "    ax[n].plot(dts,Y_data[9,:,n].detach().to('cpu'),'--')\n",
    "    ax[n].plot(dts,Y_data[999,:,n].detach().to('cpu'),'--')\n",
    "    ax[n].plot(dts[n_t],s_t[:,n].to('cpu'),'.',color='red')\n",
    "    ax[n].set_xlim([x_lim, X_lim])\n",
    "    ax[n].set_xlabel('steps')\n",
    "    ax[n].set_ylabel('Target for frequency omega '+str(n))\n",
    "    ax[n].spines['right'].set_visible(False)\n",
    "    ax[n].spines['top'].set_visible(False)\n",
    "\n",
    "    \n",
    "ax[n+1].plot(dts, torch.sum(y_data,1).to('cpu'),'black' ) \n",
    "ax[n+1].plot(dts, torch.sum(Y_data[2,:,:],1).to('cpu'),'--' ) \n",
    "ax[n+1].plot(dts, torch.sum(Y_data[9,:,:],1).to('cpu'),'--' ) \n",
    "ax[n+1].plot(dts, torch.sum(Y_data[999,:,:],1).to('cpu'),'--' ) \n",
    "ax[n+1].plot(dts[n_t],torch.sum(s_t,1).to('cpu'),'.',color='red')\n",
    "ax[n+1].set_xlabel('steps')\n",
    "ax[n+1].set_ylabel('Overall Target')\n",
    "ax[n+1].spines['right'].set_visible(False)\n",
    "ax[n+1].spines['top'].set_visible(False)\n",
    "print(a,theta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf480a",
   "metadata": {},
   "source": [
    "### SAVING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_start='Meta_Sines_Model'\n",
    "title=title_start+'_Pred.npy'\n",
    "\n",
    "Parameters=np.array( [np.array(Model.Ws.to('cpu')),np.array(Model.bs.to('cpu'))], dtype=object)\n",
    "\n",
    "np.save(os.path.join(path, title), Pred, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afba19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "\n",
    "for j in range(1,100):\n",
    "\n",
    "    n_points=j\n",
    "\n",
    "    xs_t, s_t, x_data, y_data, n_t=Sines.Sample_1(a,theta,n_points,space,Rand=False)\n",
    "\n",
    "    n_steps=1000\n",
    "\n",
    "    E_n, E_all, Y_data=MLP.Fine_Tuning(xs_t, s_t, n_steps, x_data.to('cuda'), y_data.to('cuda'))\n",
    "    Pred=np.array( [np.array(dts.to('cpu')),n_t,np.array(Y_data.to('cpu')),np.array(y_data.to('cpu')),\\\n",
    "                    np.array(E_n.to('cpu')),np.array(E_all.to('cpu')),a,theta], dtype=object)\n",
    "\n",
    "    path=\"C:\\\\Users\\lucam\\Desktop\\Meta\\Data2\"\n",
    "\n",
    "    title_start='Meta_Sines_Ex6_'+str(n_points)\n",
    "    title=title_start+'_space2.mat'\n",
    "    io.savemat(os.path.join(path, title),{\"array\": Pred})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path=\"C:\\\\Users\\lucam\\Desktop\\Meta\\Random_Examples\"\n",
    "\n",
    "\n",
    "Ex=np.arange(0,15)\n",
    "\n",
    "for n in range(np.shape(Ex)[0]):\n",
    "    \n",
    "    #path=\"C:\\\\Users\\lucam\\Desktop\\Clean\\Meta_Results\\Sines\\Second_Data\"\n",
    "    \n",
    "    path=\"C:\\\\Users\\lucam\\Desktop\\Meta\\Random_Examples\"\n",
    "    title_start='Meta_Sines_Ex'+str(Ex[n]+1)+'_10_Rand'\n",
    "    title=title_start+'.mat'\n",
    "\n",
    "    Pred=io.loadmat(os.path.join(path, title))\n",
    "    a=Pred['array'][0][6][0]\n",
    "    theta=Pred['array'][0][7][0]\n",
    "    ns=np.arange(1,11)*3\n",
    "    \n",
    "    for j in range(10):\n",
    "\n",
    "        n_points=ns[j]\n",
    "\n",
    "        xs_t, s_t, x_data, y_data, n_t=Sines.Sample_1(a,theta,n_points,space,Rand=True)\n",
    "        \n",
    "        n_steps=1000\n",
    "\n",
    "        E_n, E_all, Y_data=MLP.Fine_Tuning(xs_t, s_t, n_steps, x_data.to('cuda'), y_data.to('cuda'))\n",
    "        Pred=np.array( [np.array(dts.to('cpu')),n_t,np.array(Y_data.to('cpu')),np.array(y_data.to('cpu')),\\\n",
    "                        np.array(E_n.to('cpu')),np.array(E_all.to('cpu')),a,theta], dtype=object)\n",
    "\n",
    "        #path=\"C:\\\\Users\\lucam\\Desktop\\Clean\\Meta_Results\\Sines\\Meta0\"\n",
    "        path=\"C:\\\\Users\\lucam\\Desktop\\Meta\\Meta_single\\Data\"\n",
    "        title_start='Meta_Single_Ex'+str(Ex[n])+'_'+str(n_points)\n",
    "        title=title_start+'.mat'\n",
    "        io.savemat(os.path.join(path, title),{\"array\": Pred})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d759f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points=15\n",
    "space=2\n",
    "\n",
    "N_ex=500\n",
    "for l in range(500,500+N_ex):\n",
    "    \n",
    "    for j in range(1,41):\n",
    "        \n",
    "        n_points=j\n",
    "        rand_sample=True\n",
    "        if rand_sample==True:\n",
    "\n",
    "            a=np.zeros([Sines.N_F])\n",
    "            theta=np.zeros([Sines.N_F])\n",
    "\n",
    "            for n in range(Sines.N_F):\n",
    "\n",
    "                a[n]=np.random.uniform(Sines.As[n][0],Sines.As[n][1])\n",
    "                theta[n]=np.random.uniform(Sines.phases[n][0],Sines.phases[n][1])\n",
    "\n",
    "\n",
    "        xs_t, s_t, x_data, y_data, n_t=Sines.Sample_1(a,theta,n_points,space,Rand=True)\n",
    "\n",
    "        n_steps=1000\n",
    "\n",
    "        E_n, E_all, Y_data=MLP.Fine_Tuning(xs_t, s_t, n_steps, x_data.to('cuda'), y_data.to('cuda'))\n",
    "\n",
    "        #Pred=np.array( [np.array(dts.to('cpu')),n_t,np.array(Y_data.to('cpu')),np.array(y_data.to('cpu')),\\\n",
    "        #                    np.array(E_n.to('cpu')),np.array(E_all.to('cpu')),a,theta], dtype=object)\n",
    "        \n",
    "        \n",
    "        Pred=np.array( [np.array(E_n.to('cpu')),np.array(E_all.to('cpu')),a,theta], dtype=object)\n",
    "\n",
    "        \n",
    "        path=\"C:\\\\Users\\lucam\\Desktop\\Meta\\Random_Errors\"\n",
    "\n",
    "        title_start='Meta_Sines_Ex'+str(l)+'_'+str(n_points)\n",
    "        title=title_start+'_RandErrors.mat'\n",
    "        io.savemat(os.path.join(path, title),{\"array\": Pred})\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483c7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
